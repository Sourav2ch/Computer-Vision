{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssd mask",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rirU9STYceqLHf_xEhk0MelUauFpOfj9",
      "authorship_tag": "ABX9TyMSMyjpJX9zYw6WtShbDvBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourav56/Computer-Vision/blob/master/ssd_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hZH7-FN5MXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "80ee2118-192a-4902-a51b-05944981da57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KVh4khk5bHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "from os import getcwd\n",
        "from os import listdir\n",
        "import cv2\n",
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "import imutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEPWAyBx6zLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1f0c7f47-bab8-4409-fdd3-a013464e35a0"
      },
      "source": [
        "directory=r'/content/drive/My Drive/Colab Notebooks/facemaskdetection'\n",
        "category =  os.listdir(directory)\n",
        "print(category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['without_mask', 'with_mask']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eru0Fqx97CIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yes_im = r'/content/drive/My Drive/Colab Notebooks/facemaskdetection/with_mask'\n",
        "no_im = r'/content/drive/My Drive/Colab Notebooks/facemaskdetection/without_mask'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JnPaOc66R00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bdbe6266-f0ab-4b9a-96a5-bd1098b56e08"
      },
      "source": [
        "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir(yes_im)))\n",
        "print(\"The number of images with facemask labelled 'no':\",len(os.listdir(no_im)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of images with facemask labelled 'yes': 1915\n",
            "The number of images with facemask labelled 'no': 1918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xPNG7faCWzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "2a81cfc4-6a1c-4a22-fe3a-87c3b09015f1"
      },
      "source": [
        "\n",
        "!git clone https://github.com/prajnasb/observations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'observations'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 1638 (delta 9), reused 0 (delta 0), pack-reused 1604\u001b[K\n",
            "Receiving objects: 100% (1638/1638), 75.94 MiB | 9.35 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tiwkY3C6G_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "532fa506-72c8-4069-a6e8-e792640490c1"
      },
      "source": [
        "!ls observations/experiements/dest_folder/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  test.csv\ttrain  train.csv  val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbQjYT_O7wtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "da97bd6e-0703-4916-b39c-e98637bdda06"
      },
      "source": [
        "def data_summary(main_path):\n",
        "    \n",
        "    yes_path = main_path+'with_mask'\n",
        "    no_path = main_path+'without_mask'\n",
        "        \n",
        "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
        "    m_pos = len(listdir(yes_path))\n",
        "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
        "    m_neg = len(listdir(no_path))\n",
        "    # number of all examples\n",
        "    m = (m_pos+m_neg)\n",
        "    \n",
        "    pos_prec = (m_pos* 100.0)/ m\n",
        "    neg_prec = (m_neg* 100.0)/ m\n",
        "    \n",
        "    print(f\"Number of examples: {m}\")\n",
        "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
        "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
        "    \n",
        "augmented_data_path =  r'/content/drive/My Drive/Colab Notebooks/facemaskdetection/' \n",
        "data_summary(augmented_data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples: 3833\n",
            "Percentage of positive examples: 49.96086616227498%, number of pos examples: 1915\n",
            "Percentage of negative examples: 50.03913383772502%, number of neg examples: 1918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_jCzmUN9OSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    dataset = []\n",
        "    \n",
        "    for unitData in os.listdir(SOURCE):\n",
        "        data = SOURCE + unitData\n",
        "        if(os.path.getsize(data) > 0):\n",
        "            dataset.append(unitData)\n",
        "        else:\n",
        "            print('Skipped ' + unitData)\n",
        "            print('Invalid file i.e zero size')\n",
        "    \n",
        "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
        "    test_set_length = int(len(dataset) - train_set_length)\n",
        "    shuffled_set = random.sample(dataset, len(dataset))\n",
        "    train_set = dataset[0:train_set_length]\n",
        "    test_set = dataset[-test_set_length:]\n",
        "       \n",
        "    for unitData in train_set:\n",
        "        temp_train_set = SOURCE + unitData\n",
        "        final_train_set = TRAINING + unitData\n",
        "        copyfile(temp_train_set, final_train_set)\n",
        "    \n",
        "    for unitData in test_set:\n",
        "        temp_test_set = SOURCE + unitData\n",
        "        final_test_set = TESTING + unitData\n",
        "        copyfile(temp_test_set, final_test_set)\n",
        "        \n",
        "YES_SOURCE_DIR = \"observations/experiements/data/with_mask/\"\n",
        "TRAINING_YES_DIR = \"observations/experiements/dest_folder/train/with_mask/\"\n",
        "TESTING_YES_DIR = \"observations/experiements/dest_folder/test/with_mask/\"\n",
        "\n",
        "NO_SOURCE_DIR =  \"observations/experiements/data/without_mask/\"\n",
        "TRAINING_NO_DIR = \"observations/experiements/dest_folder/train/without_mask/\"\n",
        "TESTING_NO_DIR = 'observations/experiements/dest_folder/test/without_mask/'\n",
        "split_size = 0.8\n",
        "split_data( YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
        "split_data( NO_SOURCE_DIR,TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFYddQv7YQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d0e4e8e9-0d64-4d5d-c8d3-0e953ae8ec14"
      },
      "source": [
        "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir(\"observations/experiements/dest_folder/train/with_mask/\")))\n",
        "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir(\"observations/experiements/dest_folder/test/with_mask/\")))\n",
        "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir(\"observations/experiements/dest_folder/train/without_mask/\")))\n",
        "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir(\"observations/experiements/dest_folder/test/without_mask/\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of images with facemask in the training set labelled 'yes': 729\n",
            "The number of images with facemask in the test set labelled 'yes': 209\n",
            "The number of images without facemask in the training set labelled 'no': 1091\n",
            "The number of images without facemask in the test set labelled 'no': 213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YRKvxyQ1FMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcwnYzoc1Fcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "96da1f9e-eee1-4594-f5eb-db2e65862a4a"
      },
      "source": [
        "TRAINING_DIR = \"/content/drive/My Drive/Colab Notebooks/facemaskdetection/\"\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                    batch_size=10, \n",
        "                                                    target_size=(150, 150))\n",
        "VALIDATION_DIR = \"observations/experiements/dest_folder/test/\"\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
        "                                                         batch_size=10, \n",
        "                                                         target_size=(150, 150))\n",
        "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3833 images belonging to 2 classes.\n",
            "Found 422 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_HuOEB1FkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "ede9a70f-6d99-4944-cace-f51bada39535"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=10,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 22/384 [>.............................] - ETA: 27:18 - loss: 0.8829 - acc: 0.5045"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "384/384 [==============================] - ETA: 0s - loss: 0.4650 - acc: 0.7949INFO:tensorflow:Assets written to: model-001.model/assets\n",
            "384/384 [==============================] - 1635s 4s/step - loss: 0.4650 - acc: 0.7949 - val_loss: 0.4267 - val_acc: 0.8673\n",
            "Epoch 2/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4013 - acc: 0.8482INFO:tensorflow:Assets written to: model-002.model/assets\n",
            "384/384 [==============================] - 419s 1s/step - loss: 0.4013 - acc: 0.8482 - val_loss: 0.3600 - val_acc: 0.8649\n",
            "Epoch 3/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.3441 - acc: 0.8654INFO:tensorflow:Assets written to: model-003.model/assets\n",
            "384/384 [==============================] - 420s 1s/step - loss: 0.3441 - acc: 0.8654 - val_loss: 0.2851 - val_acc: 0.8910\n",
            "Epoch 4/10\n",
            "384/384 [==============================] - 412s 1s/step - loss: 0.3321 - acc: 0.8669 - val_loss: 0.3042 - val_acc: 0.8744\n",
            "Epoch 5/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.3170 - acc: 0.8748INFO:tensorflow:Assets written to: model-005.model/assets\n",
            "384/384 [==============================] - 419s 1s/step - loss: 0.3170 - acc: 0.8748 - val_loss: 0.2575 - val_acc: 0.8981\n",
            "Epoch 6/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.2976 - acc: 0.8865INFO:tensorflow:Assets written to: model-006.model/assets\n",
            "384/384 [==============================] - 414s 1s/step - loss: 0.2976 - acc: 0.8865 - val_loss: 0.2450 - val_acc: 0.9100\n",
            "Epoch 7/10\n",
            "384/384 [==============================] - 411s 1s/step - loss: 0.3094 - acc: 0.8737 - val_loss: 0.3821 - val_acc: 0.8578\n",
            "Epoch 8/10\n",
            "384/384 [==============================] - 411s 1s/step - loss: 0.3057 - acc: 0.8808 - val_loss: 0.2820 - val_acc: 0.8957\n",
            "Epoch 9/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.3085 - acc: 0.8805INFO:tensorflow:Assets written to: model-009.model/assets\n",
            "384/384 [==============================] - 412s 1s/step - loss: 0.3085 - acc: 0.8805 - val_loss: 0.2256 - val_acc: 0.9052\n",
            "Epoch 10/10\n",
            "384/384 [==============================] - 412s 1s/step - loss: 0.3235 - acc: 0.8782 - val_loss: 0.2306 - val_acc: 0.8957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHcM70_01Fgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsvCgJZKdIwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/trainHistoryDict', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmF-3egyZCKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlL2u8eagfQG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "4c7ae8b7-e920-4446-e0e8-a6472616acc8"
      },
      "source": [
        "!wget https://github.com/AKSHAYUBHAT/TensorFace/raw/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat\n",
        "!wget https://github.com/ageitgey/face_recognition_models/raw/master/face_recognition_models/models/dlib_face_recognition_resnet_model_v1.dat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 20:44:40--  https://github.com/AKSHAYUBHAT/TensorFace/raw/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AKSHAYUBHAT/TensorFace/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2020-08-26 20:44:40--  https://raw.githubusercontent.com/AKSHAYUBHAT/TensorFace/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   156MB/s    in 0.6s    \n",
            "\n",
            "2020-08-26 20:44:42 (156 MB/s) - ‘shape_predictor_68_face_landmarks.dat’ saved [99693937/99693937]\n",
            "\n",
            "--2020-08-26 20:44:42--  https://github.com/ageitgey/face_recognition_models/raw/master/face_recognition_models/models/dlib_face_recognition_resnet_model_v1.dat\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ageitgey/face_recognition_models/master/face_recognition_models/models/dlib_face_recognition_resnet_model_v1.dat [following]\n",
            "--2020-08-26 20:44:42--  https://raw.githubusercontent.com/ageitgey/face_recognition_models/master/face_recognition_models/models/dlib_face_recognition_resnet_model_v1.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22466066 (21M) [application/octet-stream]\n",
            "Saving to: ‘dlib_face_recognition_resnet_model_v1.dat’\n",
            "\n",
            "dlib_face_recogniti 100%[===================>]  21.42M  52.6MB/s    in 0.4s    \n",
            "\n",
            "2020-08-26 20:44:43 (52.6 MB/s) - ‘dlib_face_recognition_resnet_model_v1.dat’ saved [22466066/22466066]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85vAi7PMhLV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d3229bdf-1c8d-4c11-bf8e-d26658a5a7a8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-26 20:45:32--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "haarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-08-26 20:45:32 (12.4 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRH3ZLiAgwHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "36f56f42-15a6-4de5-a57a-ac1f59972395"
      },
      "source": [
        "import numpy as np, cv2, dlib, os, pickle\n",
        "path = './facedata/'\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "sp = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
        "FACE_DESC = []\n",
        "FACE_NAME = []\n",
        "for fn in os.listdir(path):\n",
        "    if fn.endswith('.jpg'):\n",
        "        img = cv2.imread(path + fn)[:,:,::-1]\n",
        "        dets = detector(img, 1)\n",
        "        for k, d in enumerate(dets):\n",
        "            shape = sp(img, d)\n",
        "            face_desc = model.compute_face_descriptor(img, shape, 100)\n",
        "            FACE_DESC.append(np.array(face_desc))\n",
        "            print('loading...', fn)\n",
        "            FACE_NAME.append(fn[:fn.index('_')])\n",
        "pickle.dump((FACE_DESC, FACE_NAME), open('trainset.pk', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ad8c1495e265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mFACE_DESC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mFACE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './facedata/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABG3qGpn1FBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "02362427-8b52-4baa-926d-a50e4ae36de4"
      },
      "source": [
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "detector = './facedata/'\n",
        "sp = dlib.shape_predictor('')\n",
        "model = dlib.face_recognition_model_v1('')\n",
        "FACE_DESC, FACE_NAME = pickle.load(open('', 'rb'))\n",
        "\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "while True:\n",
        "    byte = eval_js('capture()')\n",
        "    frame = byte2image(byte)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        img = frame[y-10:y+h+10, x-10:x+w+10][:,:,::-1]\n",
        "        dets = detector(img, 1)\n",
        "        for k, d in enumerate(dets):\n",
        "            shape = sp(img, d)\n",
        "            face_desc0 = model.compute_face_descriptor(img, shape, 1)\n",
        "            d = []\n",
        "            for face_desc in FACE_DESC:\n",
        "                d.append(np.linalg.norm(np.array(face_desc) - np.array(face_desc0)))\n",
        "            d = np.array(d)\n",
        "            idx = np.argmin(d)\n",
        "            if d[idx] < 0.5:\n",
        "                name = FACE_NAME[idx]\n",
        "                #print(name)\n",
        "                cv2.putText(frame, name, (x, y-5), cv2.FONT_HERSHEY_COMPLEX, .7, (255,255,255),2)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(frame)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2c8e329e9a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mface_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_recognition_model_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFACE_DESC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFACE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol4oazyNy9an",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4GuE9u-y9X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiood_gTx-5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}